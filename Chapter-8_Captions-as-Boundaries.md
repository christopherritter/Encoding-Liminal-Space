---
title: "ENCODING LIMINAL SPACE"
subtitle: "A Technical Manual for Reality Engineering"
author: "Geddon Labs Research Division"
classification: "Threshold Operations"
---

# CHAPTER 8: CAPTIONS AS BOUNDARIES

## Language as Encoded Instruction

You have gathered raw territory—patterns, images, representations of your defined space. You have translated this territory into latent form, compressing semantic richness into the mathematical substrate where learning occurs. Now you arrive at the precise threshold where **intention crystallizes into specification**, where human meaning becomes machine-readable boundary. This threshold is the caption. In this chapter, you learn that captions are not labels appended to data. Captions are the linguistic boundary through which you encode territorial definition into the system's learning process itself.

The caption operates at the exact point of translation between two domains. It stands at the boundary between what you intend for the system to learn and what the system is capable of learning. The precision with which you translate intention into caption determines whether the system's learning will crystallize your territorial definition with coherence or dissolve into ambiguity. This is not metaphor. This is the operational substrate through which meaning becomes learned prior. You do not describe images and hope the system understands them. You specify linguistic boundaries that constrain which learned patterns will activate during training and which will remain dormant. The caption is the threshold act through which you perform this specification. Every word you write in a caption becomes a coordinate in semantic space. Every word you omit becomes a boundary the system will not learn to navigate. This chapter teaches you to write captions as conscious acts of territorial encoding.

---

## 8.1 The Caption as Specification of Meaning

A caption translates human intention into computational parameters. This is not description of external reality. This is encoding of boundaries that the system will learn as its territory. When you write a caption, you are not narrating what is objectively present in an image. You are **specifying which semantic coordinates in latent space will activate during training for this particular image-caption pair**. This specification becomes the learned prior—the statistical pattern the model will internalize as legitimate association.

Technically, the caption operates through the **embedding process**. When the system processes your caption during training, it tokenizes your language—breaking it into semantic units—and maps these tokens into latent vectors. These vectors become the **guidance** through which the image-to-meaning association is learned. If your caption is vague ("a picture"), the tokens produced are diffuse, spanning wide regions of semantic space. The image becomes associated with broad, undifferentiated learned priors. If your caption is precise ("a weathered red barn with horizontal siding, iron hinges corroded orange, standing alone in winter wheat stubble under slate-gray sky"), the tokens are sharply differentiated, occupying specific coordinates. The image becomes associated with dense, bounded learned priors—the system learns to recognize precise semantic coherence between visual pattern and linguistic specification.

This distinction is mechanistic. During training, the model learns a function that maps images to their captions through an embedding space. The tighter the semantic correspondence between caption and image, the more stable the learned function becomes in that region of space. The looser the correspondence—the more vague or contradictory the caption—the more the learned function becomes unstable, oscillating between competing interpretations. When you later generate output with a prompt aligned to that territory, the system activates those learned priors. If they are sharp and coherent, output manifests with precision. If they are diffuse or contradictory, output manifests with ambiguity or collapse.

The metaphysical parallel is direct. When you perceive an object, you simultaneously perceive linguistic category. You recognize not merely visual pattern but meaning—a coffee cup is not abstract brown cylinder but an object associated with warmth, morning ritual, specific hand-position, particular temporal context. This meaning shapes what you perceive. If your linguistic categorization is imprecise ("a drink vessel"), your perception remains diffuse—you notice warmth, weight, cylindrical form but not the specificity that makes the cup coherent. If your linguistic categorization is precise ("a chipped ceramic mug with a handle worn smooth, containing cooling coffee, positioned on a desk beside morning notes"), your perception crystallizes. Details that were background noise now foreground. Coherence emerges from precision. The mechanism is identical. You crystallize meaning through linguistic specification. The system crystallizes learned prior through caption precision. Both operations encode intention into the substrate through language. The principle is unified.

---

## 8.2 Language Collapse and Semantic Precision

Imprecise language leads to ambiguity in the system's learning. Semantic precision collapses possibility into specific manifestation. This mechanism operates directly at the boundary between superposed potential and crystallized form.

When you write a vague caption ("plant"), you deploy a token that spans multiple regions of latent space simultaneously. The word "plant" carries superposed meaning: orchid plant, shrub plant, herb plant, potted plant, plant growing in soil, plant floating in water, plant as verb. During training, this ambiguity enters the learned parameters. The image becomes associated with the entire superposition of meanings. When the system later encounters a prompt containing "plant," it has learned to activate all competing regions simultaneously. The generated output oscillates between interpretations. Details hallucinate inconsistently. Coherence breaks because the learned prior contains competing specifications.

When you write a precise caption ("moss-covered river stone shaped like a plant, with delicate water droplets beading on its weathered surface"), you deploy tokens that occupy specific coordinates in semantic space. Each token constrains the others. "Moss-covered" specifies the surface texture, excluding smooth or glossy plant forms. "River stone" specifies material and origin, excluding soil plants or potted plants. "Delicate water droplets" specifies surface treatment. The caption becomes a precise path through semantic space. During training, this path enters the learned parameters as a stable coordinate. The image becomes associated with unambiguous specification. When the system later generates output guided by this specification, it activates this precise region. Details manifest consistently. Coherence emerges.

Language collapse operates through cumulative specification. A single word exists in superposition until adjacent words collapse it. "Plant" is ambiguous until "moss-covered" specifies vegetation type. "Moss-covered" is ambiguous until "river stone" specifies substrate. "River stone" is ambiguous until "delicate water droplets" specifies temporal state (wet, recently exposed). Each word collapses the superposition of the previous word, converging toward a single crystallized meaning.

This is how the system learns to **collapse possibility into manifestation through linguistic boundary**. The caption is the mechanism. The precision of the caption determines how successfully possibility collapses. A vague caption fails to collapse. Superposition remains. Ambiguity persists. A precise caption collapses possibility decisively. Superposition resolves. Manifestation becomes coherent.

This is also how consciousness operates. When you encounter an ambiguous visual stimulus—a shape in dim light that might be human or object—your perception remains in superposition. Multiple interpretations compete simultaneously. Then linguistic specification arrives: "It's a statue." The ambiguous form collapses. Your perception reorganizes. What you perceive is now statue, not human possibility. The collapse was instantaneous, governed by linguistic boundary. When the caption is precise, collapse is decisive. When the caption is ambiguous, perception oscillates. The mechanism is identical. Language collapses superposition into coherent experience. Captions collapse superposed learned priors into coherent manifestation. The principle is unified. The substrate differs. The operation is the same.

---

## 8.3 Captions Define What the System Learns to See

The system does not learn to recognize universal categories. The system learns to recognize what its training captions have specified. Captions define the system's perceptual territory.

Consider the concept "weathered." In the world, weathering occurs across infinite variations: rust oxidation, wood grain raising, fabric fading, paint peeling, stone surface pitting, metal tarnishing, leather cracking. Without caption specification, the system cannot distinguish these variations. It must learn what "weathered" means from how you caption weathered things. If you caption all weathered images as simply "old," the system learns to associate weathering with vague temporal passage. It develops diffuse learned priors for temporal decay. If you caption the same images with precise specifications—"rust oxidation on iron hinge," "raised wood grain on cedar plank," "faded indigo on cotton cloth"—the system learns to recognize weathering as a precise pattern specific to material and chemistry. It develops sharp, bounded learned priors for material-specific decay.

This extends through all semantic categories. The system's capacity to "see" texture, emotion, temporal state, spatial relationship, movement, abstraction—all of these depend entirely on how you caption your training data. If you caption emotional faces as "sad face," the system learns a vague association between facial pattern and emotional category. It develops a superposed learned prior that conflates multiple expressions under a single label. If you caption the same images with precision—"downturned mouth, eyes squinted, forehead wrinkled in concentrated sadness," or "mouth open slightly, eyes distant, face turned away in withdrawn sadness"—the system learns to distinguish emotional substates. It develops sharp, specific learned priors that separate different manifestations of sadness.

This is not limitation. This is the mechanism through which the system becomes a specialist in your territory. Generic captions produce generic learned priors. The system becomes a generalist, capable of manifesting broad categories but unable to inhabit specific territory with precision. Precise, differentiated captions produce precise, bounded learned priors. The system becomes a specialist in your territory, capable of manifesting nuanced variations within defined semantic space.

In consciousness, you develop specificity in perception through linguistic specification and intentional attention. A person trained only to observe weather learns to distinguish cloud types, wind patterns, atmospheric pressure effects with precision impossible for untrained observation. The training is linguistic—names, categories, specifications—combined with repeated perceptual engagement. The trained observer's consciousness manifests different perception than untrained observation. Not because the sky contains more information, but because training has specified how to carve possibility into coherent perceptual categories. The system learns analogously. Your captions are the training specification. The system's learned priors are the developed perception. What the system learns to see is exactly what your captions have taught it to recognize.

---

## 8.4 Naming as Threshold Act

The act of naming in the caption is the act of crossing the boundary between conceptual space and computational reality. This is not poetic assertion. This is operational description.

Before naming occurs, the image exists as raw data—pixels arranged in mathematical arrays, containing infinite interpretable patterns. The image has not yet become a learned object. It exists in potential, superposed across all possible meanings. Naming through the caption performs the threshold crossing. When you write the caption, you specify which coordinates in semantic space this image occupies. You encode a specific position within the system's latent territory. The image transitions from raw potential to named specification, from undifferentiated data to positioned meaning.

Technically, naming operates through tokenization and embedding. When you write a caption, each word becomes a token—a discrete semantic unit mapped to latent coordinates. The sequence of tokens defines a path through semantic space. During training, the image becomes anchored to this path. The image-caption pair becomes a learned association. The system internalizes: when encountering patterns similar to this image, activate the semantic coordinates this caption has specified. This internalization is learned prior. It is permanent. The image has been named, and this naming has crystallized a position within the territory.

But naming is not arbitrary labeling. Naming is boundary-making. When you name an image "moss-covered river stone," you create a boundary distinguishing this image from images named "polished river stone" or "moss-covered forest floor" or "water-worn pebble." This boundary becomes real within the system's learned territory. The system learns to activate different learned priors for each named category. The boundaries you create through naming become the coordinates the system navigates.

This is how consciousness actually manifests reality. Before naming, sensory data remains undifferentiated superposition. The phenomenal field is chaotic, lacking structure. Naming—linguistic categorization—performs the threshold crossing. When you name the stimulus "cup," it transitions from undifferentiated sensation to specific object. The boundary becomes real. Your consciousness no longer perceives generic form. It perceives cup-specifically. The learned priors of your consciousness activate. You now expect warmth, capacity for liquid, hand-sized structure. You move toward the cup for drinking. Consciousness has manifested a specific object through naming. The naming was not arbitrary. The boundary was not mere label. The naming was the crystallization of a specific position within your perceptual territory.

Naming in the caption is the identical operation enacted on computational substrate. The caption is the act through which you encode a specific position within the system's latent space. The image becomes named. Boundary becomes real. A position within territory is crystallized. The system learns this boundary. Manifestation becomes bound by it. Your reality engineering operates precisely here—at the threshold where naming and learning coincide, where language and learned prior become identical operation, where you cross the boundary from intention into specification through the act of encoding territorial definition through language. The caption is that threshold. The naming is the crossing. The manifestation is the consequence of this crossing, crystallizing structure from infinite possibility through the power of linguistic boundary.

---

# PRACTICAL PROTOCOLS AND EXPERIMENTATION

## Practical Protocol 8.1: Caption Strategy Development

You develop caption strategy by first clarifying your territorial intention, then designing language that encodes this intention as precise learned prior. This protocol ensures that your captions function as conscious boundary-making rather than generic labeling.

**Step 1: Clarify Territorial Specification**

Before writing captions, define precisely what you want the system to learn as your territory's core specifications. This is not vague wish—this is exact boundary definition. For each semantic category within your territory, write 3-5 precise descriptions. Example: If your territory is "abandoned architectural spaces," clarify your specific interest: Are you focused on the visual aesthetic of decay? The temporal markers of abandonment? The spatial relationships within empty structures? The emotional resonance of absence? Define this precisely.

**Territory Clarification Template:**
- Core concept: *e.g., "abandoned architectural spaces"*
- Specific focus: *e.g., "visual markers of temporal decay, not emotional narrative"*
- Critical variables (the semantic dimensions you want the system to distinguish): *e.g., material type, decay pattern, spatial emptiness, architectural style, atmospheric condition*

**Step 2: Establish Caption Density Parameters**

Decide the **Token Length** and **Semantic Density** appropriate to your territory and training architecture. Token Length refers to approximate word count per caption. Semantic Density refers to how many distinct semantic variables you pack into each caption.

For most diffusion-based systems: **8-15 tokens (approximately 10-20 words) represents optimal balance between specificity and training stability**. Below 8 tokens, most captions become too vague to encode precise learned prior. Above 20 tokens, many architectures begin to degrade token importance (later tokens receive less gradient signal). Optimal range varies by architecture—experiment within this range for your specific setup.

Semantic Density represents how many distinct semantic dimensions you specify. Example: "a red barn" = 2 dimensions (color, building type). "A weathered red barn with collapsed roof standing in winter wheat" = 5 dimensions (material state, color, building type, structural damage, seasonal context, landscape). Higher Semantic Density produces sharper learned priors when architectural capacity supports it. Lower Semantic Density produces more flexible learned priors but with less specificity.

**Density Parameter Selection:**
- Choose your **Token Length** target (recommend 12-16 tokens for standard diffusion systems)
- Choose your **Semantic Density** level: Conservative (2-3 dimensions), Balanced (4-5 dimensions), Saturated (6+ dimensions)
- Note: Conservative approach suits complex territories where you want flexibility. Saturated approach suits well-defined territories where precision matters more than variation.

**Step 3: Design Caption Patterns**

Create 3-5 caption patterns that encode your territorial specifications. These are template structures you'll adapt for specific images. Patterns should emphasize your critical variables and maintain consistent semantic structure.

**Caption Pattern Examples for "Abandoned Architecture":**

Pattern A (Material-focused): "[Material condition] [building element] with [decay pattern], [spatial context]"
- Example: "weathered wooden door frame with peeling paint and exposed wood grain, standing in empty hallway lit by distant window"

Pattern B (Temporal-focused): "[Architectural style] [building type], [temporal marker], [material evidence of age]"
- Example: "Victorian-era warehouse, abandoned approximately fifty years, concrete floor cracked and colonized by vegetation growth"

Pattern C (Spatial-focused): "[Spatial configuration], [architectural elements], [atmospheric condition], [evidence of time]"
- Example: "Large open floor space with exposed ceiling joists, industrial windows darkened by grime, dust visible in angled light"

Develop patterns that emphasize YOUR specific territorial focus. These patterns maintain consistency—the system learns that certain semantic structures are characteristic of your territory.

**Step 4: Write Precise Captions for Your Dataset**

For each image in your training set, write a caption following your established patterns and parameters. Ensure that:

- Each caption targets 12-16 tokens (adjust for your chosen parameters)
- Each caption emphasizes your critical variables
- Each caption avoids generic language ("beautiful," "interesting," "good")
- Each caption specifies observable, differentiated detail rather than subjective interpretation

**Examples of weak vs. strong captions for same image:**

*Weak:* "An old abandoned building"  
*Reason:* Too vague, no distinctive variables, fails to encode territorial precision

*Strong:* "Brick industrial building with boarded windows, roof sagging under accumulated weather, brick facade stained with decades of mineral runoff"  
*Reason:* Specifies material, structure type, damage pattern, temporal evidence, aesthetic markers

---

## Experimentation 8.1: Testing Caption Precision

You test how caption precision impacts manifestation stability by training variants of your system with three distinct caption types applied to the same dataset. This experimentation reveals the direct relationship between linguistic precision and learned prior coherence.

**Experimental Design:**

Prepare three versions of your training dataset, all containing the same images. Vary only the captions assigned to each image. Keep all other training parameters identical (learning rate, batch size, epochs, model architecture).

**Dataset Version 1: Vague/Short Captions**

For each image, write a single generic descriptor (2-4 tokens max). Emphasize minimal information.

Examples:
- "An abandoned building"
- "Architectural interior"
- "Old structure"
- "Empty space"

Purpose: Establish baseline for minimal semantic specification. These captions remain highly superposed across semantic space. Learned priors should be diffuse, producing high variation in generated outputs.

**Dataset Version 2: Precise/Descriptive Captions**

For each image, write detailed captions following your developed caption strategy (12-16 tokens, 4-5 semantic dimensions).

Examples:
- "Industrial brick building, boarded windows dark, roof sagging from accumulated weather, facade stained with mineral runoff from decades of exposure"
- "Concrete floor of empty warehouse, cracked surface with botanical colonization, natural light from high industrial windows creating architectural shadow patterns"

Purpose: Establish optimized precision. These captions occupy specific coordinates in semantic space. Learned priors should be sharp and bounded, producing consistent manifestation with low variation.

**Dataset Version 3: Highly Complex/Contradictory Captions**

For each image, write captions that layer multiple semantic registers or include subtle contradictions (20+ tokens, attempting to encode 8+ semantic dimensions simultaneously).

Examples:
- "Brutalist industrial architecture simultaneously conveying both abandonment and active occupation through paradoxical spatial configuration, temporally ambiguous between 1960s modernism and contemporary decay aesthetic"
- "Weathered wooden structure embodying contradictory states of structural fragility and monumental permanence, spatial emptiness paradoxically filled with architectural presence"

Purpose: Test over-specification. These captions exceed typical architectural parsing capacity. Learned priors should become unstable, producing distorted or incoherent manifestation.

**Training Procedure:**

Train three separate instances of your system, each using one dataset version. Maintain identical hyperparameters:
- Same learning rate
- Same batch size
- Same number of epochs
- Same validation procedure

**Measurement Protocol:**

For each trained model, generate 10-15 outputs using consistent prompt specifications related to your territory. Document the following metrics:

| Metric | Method |
|--------|--------|
| **Coherence Score** | Rate each output 1-5 for internal visual/conceptual consistency (all elements relate coherently to territorial definition). Average across all outputs. |
| **Semantic Alignment** | Rate each output 1-5 for how well it manifests the specific semantic categories you specified in captions (e.g., if captions emphasize "material decay," does output show clear decay markers?). Average across outputs. |
| **Variation Range** | Generate 10 outputs with identical prompt, measure pixel-level difference between outputs (standard deviation of pixel values across outputs). Higher = more variation. |
| **Stability** | Rate consistency of core territorial elements across the 10 generated outputs. Do the same semantic markers appear reliably, or do they change drastically? Rate 1-5. |
| **Distortion Markers** | Document presence of visual artifacts, hallucinated details, or incoherent elements (e.g., duplicated features, impossible geometry, semantic contradiction). Count frequency. |

**Data Analysis:**

Create a comparison table showing results across all three dataset versions. Expected findings:

- **Version 1 (Vague):** Lower Coherence, lower Semantic Alignment, high Variation, lower Stability
- **Version 2 (Precise):** Highest Coherence, highest Semantic Alignment, moderate Variation, highest Stability, minimal Distortion
- **Version 3 (Complex):** Moderate to high Distortion, incoherent variation, lower Stability, potential semantic collapse

*Reflection prompt:* **Which level of linguistic complexity most effectively crystallized your intention? Record the relationship between caption length and output coherence. At what point did additional semantic density stop improving coherence and begin producing instability? Document the precise token length and semantic dimension count that produced your optimal manifestation.**

---

## Documentation 8.1: Caption Effectiveness Log

Maintain this log throughout your captioning process and ongoing experimentation. The log tracks how linguistic precision correlates with manifestation quality. Document each caption iteration and the consequent system behavior.

**Seven Required Elements for Each Entry:**

**1. Date, Time, Environment Setup**

Record when the caption work occurred and any relevant environmental or computational conditions that might affect learning.

Example:
```
Date/Time: November 10, 2025, 2:30 PM EST
Environment: Local GPU setup, ambient temperature 68°F
Computational state: Model initialized from checkpoint_epoch_12
```

**2. Operational Objective**

Define the specific boundary you're encoding through this caption iteration. What semantic territory are you trying to specify? What distinction are you trying to encode?

Example:
```
Objective: Distinguish between "architectural decay" specifically 
through material processes (weathering, oxidation, colonization) 
versus generic "old building" categorization. Create learned prior 
that recognizes material-specific temporal markers.
```

**3. Parameters Adjusted**

Record the specific caption parameters you modified:

- **Caption Length** (token count or word count target)
- **Semantic Density** (number of distinct semantic dimensions encoded)
- **Specificity Level** (generic/moderate/precise/saturated)
- **Pattern used** (reference your established caption patterns)

Example:
```
Caption Length: 14 tokens (target)
Semantic Density: 5 dimensions (material, decay type, architectural element, temporal marker, spatial context)
Specificity: Precise (emphasizing observable detail over subjective interpretation)
Pattern: Material-Decay Pattern with explicit temporal evidence
```

**4. Expectation**

Before training or generation, predict what you expect to observe. What coherence level do you predict? What semantic features should manifest? What variations do you anticipate?

Example:
```
Expected: System should distinguish material-specific decay patterns in generated outputs. 
When prompted "weathered brick," should show oxidation patterns, mineral runoff, surface texture variation.
When prompted "abandoned wooden structure," should show wood grain raising, paint peeling, structural settling.
Expected Coherence: 4-5/5 (high consistency within material category)
Expected Variation: Moderate (different architectural contexts, but consistent material behavior)
```

**5. Actual Outcome**

After training or generation using the new captions, document what actually manifested.

Example:
```
Actual Result: Generated outputs show clear material differentiation.
Brick outputs display oxidation, mineral staining, surface weathering textures.
Wooden outputs show raised grain, peeling paint layers, structural settlement.
Coherence observed: 4.2/5 average across 15 outputs
Variation: Moderate (consistent material behavior across different contexts)
Distortion artifacts: Minimal (occasional hallucinated details in window frames)
```

**6. Surprise or Divergence**

Where did actual outcome diverge from expectation? Where did language fail to encode your intention precisely? What unexpected patterns emerged?

Example:
```
Surprise 1: Captions emphasizing "decades of exposure" produced temporal markers 
more effectively than I predicted. System learned to encode specific weathering 
depth, not just generic "old."

Surprise 2: When I used phrase "botanical colonization," system over-specified 
vegetation presence, treating it as mandatory rather than contextual. Next iteration 
should reframe as "potential for vegetation growth" rather than assertion.

Divergence: Expected higher variation across architectural styles. 
Actual outputs showed material consistency across styles—system prioritized 
material specification over architectural diversity. This is useful but narrower 
than expected territory.
```

**7. Interpretation / Next Iteration**

Synthesize observations into refined understanding. What did this caption test teach you? How will you refine captions in the next iteration?

Example:
```
Interpretation: Linguistic precision at 12-16 tokens with 5 semantic dimensions 
produces optimal coherence for material-specific decay territory. System successfully 
encodes material-specific learned priors. Token length beyond 18 begins degrading 
architectural variation—suggests optimal saturation point at current level.

Next Iteration: Increase emphasis on architectural style distinction within 
material category. Reframe botanical elements as contextual rather than mandatory. 
Test whether expanding token length to 16-18 while reducing semantic dimensions 
from 5 to 4 maintains coherence while increasing architectural variation.

Refined Caption Pattern: "[Material type] with [specific decay process], 
[architectural context], [temporal evidence in material]"
```

---

**Log Template for Multiple Entries:**

Use this structure for each caption iteration or experimentation cycle:

```
=== CAPTION EFFECTIVENESS LOG ENTRY ===

Date/Time: [when the work occurred]

Operational Objective: [the specific boundary you encoded]

Parameters Adjusted:
- Caption Length: [tokens/words]
- Semantic Density: [dimensions]
- Specificity: [level]
- Pattern: [which pattern used]

Expectation: [what you predicted would happen]

Actual Outcome: [what actually manifested]

Surprise/Divergence: [where language diverged from intention]

Interpretation/Next: [what you learned; how you'll refine]

---
```

Maintain this log continuously. Review quarterly to identify patterns in how caption precision correlates with manifestation coherence. The log becomes your primary instrument for understanding how to encode territorial definition through language with increasing precision.

---

# SYNTHESIS: THE CAPTION AS THRESHOLD OPERATION

You have arrived at the exact point where human intention becomes machine specification. You have learned that captions are not mere labels—they are the linguistic boundary through which territorial definition enters the system's learned substrate. The precision with which you write captions determines the coherence with which the system will manifest your territory.

The mechanism operates identically across registers. Technically, captions function as semantic coordinates that guide learned prior formation during training and inference. They collapse superposed possibility into specific manifestation through accumulated linguistic precision. Metaphysically, naming through caption performs the identical act of boundary-crossing—the moment when undifferentiated potential becomes crystallized form.

Your next threshold is training itself—the moment when your specified territory enters the system's permanent parameters, when captions become crystallized learned priors, when your boundary definitions become the system's operational reality.

---

**Next: Chapter 9 — Caption Strategies by Architecture**
*System-Specific Communication and Optimized Linguistic Protocols*